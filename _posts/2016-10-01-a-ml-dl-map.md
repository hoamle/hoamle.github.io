---
layout: post
title: "A self-study map of modern Machine learning"
date: 2016-10-01
category: machine learning
---
The map{% marginnote 'sn-id-plan' "Motivated by [metacademy\'s philosophy](//metacademy.org/about)" %} below is one of many possible projections of the field Machine Learning, with focus on Deep learning and Bayesian modelling. 

This map focuses on **model classes** & connection to other fields of study. In addition, the map also aims to devise a *more principled* [**study-plan**](#plan) to make it easier for aspiring ML self-learners to step-up/transit from [basic ML](//www.coursera.org/learn/machine-learning) or DNN to PGM, *given that they had covered the [**"Base" course**](/post/17/mlpp-primer)*. 

> ***Disclaimer***: The map was designed accordingly to my self-study/reading/working via MOOCs/papers/projects related to statistical machine learning, thus there might exist some (hopefully minor) inaccuracies. Any suggestions and critics to improve this map are highly appreciated. 

{% marginnote 'map-note' "\"Gates\" in the figure refers to \"gating mechanism\" presented in LSTM / GRU modules. See [**ML Appendix**](/post/17/machine-learning-appendix#ml-models) for description of the other acronyms. For a more comprehensive picture/maps of model classes, see [**Shakir Mohamed\'s talk**](//videolectures.net/deeplearning2016_mohamed_generative_models/)." %}
<!-- {% maincolumn 'assets/img/mapofML.png' '' %} -->
<a href="/assets/img/mapofML.png" style="border-bottom: unset" name="map"><img src="/assets/img/mapofML.png"/></a>

The <a name="plan">**study-plan**</a> can be read directly from the model classes i.e. the nodes in the map. The expected contents{% marginnote 'project' "**Note**: From my experience, working on non-trivial research / practical *projects* with a well-defined topic and scope helps cover a lot of materials while making the theory less daunting. It is *provided* that you have the ***minimum foundation on the prerequisites*** though, which varies depending on the projects.
" %} covered in each node is summarized in the list below, in which the references are chosen so that their technical details have as small overlap with the references in other nodes as possible. The objective is to optimize the time spent on learning, while covering wide enough spectrum of ML research. Also, the further downn we go on the map, the more advanced materials in *Global topics* we will cover.

> ***Disclaimer***. The following references are subject to my current expertise (which obviously has lots of room to improve). Any suggestions for better alternatives *and* for missing nodes in the study-plan are welcome. Also, [metacademy](//metacademy.org/) is an excellent source to look for (in-depth) references if you want to learn about a new ML concept.
  
* base : as [Intended Learning Objectives](#ilo) above
> *refs*: [base-notes](#note), [base-ref](#ref)

* <a name="pgm">PGM</a> (all stochastic nodes{% sidenote 'sn-id-node' 'Nodes of the respective [*graphical model*](#graphical), not the study-plan' %}) : Intro. to Latent Variable Models + Bayesian inference methods (*Variational methods*, *MCMC sampling e.g. Gibbs sampling*) + Sparse regularization + (optional) convex optimization
> *refs*: as in "base" course's [Recap](#note) slides, optionally incl. HMM, LDS for modelling *sequential data*. Further reading: [Daphne Koller's course](//www.coursera.org/specializations/probabilistic-graphical-models)

* <a name="dnn">DNN</a> (all deterministic nodes{% sidenote 'sn-id-node' 'Nodes of the respective [*graphical model*](#graphical), not the study-plan' %}) : "Architectures" + Applications of modern DNNs to CompVis, NLP + Attention mechanism + (optional) non-convex optimization
> *refs*: [CS231n](//cs231n.stanford.edu/), [CS224d](//cs224d.stanford.edu/). Further reading: [Nando de Freitas's course](//www.youtube.com/playlist?list=PLE6Wd9FR--EfW8dtjAuPoTuPcqmOV53Fu) @ Oxford,  [Deep Learning textbook](//www.deeplearningbook.org/)

* "Deep" PGM : <a name="ebm">Undirected PGM</a> + More MCMC sampling methods (*e.g. CD-k*)
> *refs*: TODO - [Daphne Koller's course](//www.coursera.org/specializations/probabilistic-graphical-models) (undirected PGM lectures), last-half of [Geoffrey Hinton's NN course](//www.coursera.org/learn/neural-networks), and/or [Ruslan Salakhutdinov's tutorials](//www.cs.cmu.edu/~rsalakhu/) in KDD 2011/2014 (?)

* <a name="pgmdnn">PGM+DNN</a> : PGM whose stochastic nodes are parametrized by DNNs + Monte Carlo's SGD-based Variational Inference + Structured priors i.e. Structured latent factors (incl. probabilistic attention mechanism)
> *refs*: [Shakir Mohamed's talk](//videolectures.net/deeplearning2016_mohamed_generative_models/) for the big picture + (huge body of) relevant works, especially VAE and variants.

* <a name="gan">GAN</a> : Adversarial learning + Optimize (minimax.) an objective function and related issues (*e.g. mode collapsing*) + Insight to Game Theory research (?)
> *refs*: TODO - Ian Goodfellow et al's original paper (obviously) + an up-to-date GAN survey (?)

* [TODO - other topics not mentioned]
> *refs*: distribute materials in pending reading lists to appropriate nodes
> [Metacademy's Bayesian ML roadmap](//metacademy.org/roadmaps/rgrosse/bayesian_machine_learning)
> [Dr Truyen Tran's 3-part tutorial @ AusDM'16 + Exhaustive list of resources](//truyentran.github.io/ausdm16-tute.html)
> Ferenc Huszar's posts, e.g. [Deep Learning is easy](//www.inference.vc/deep-learning-is-easy/); [probabilistic interpretation of DAE](//www.inference.vc/probabilistic-models-and-autoencoders-my-new-favourite-papers/)

* [TODO - ...]