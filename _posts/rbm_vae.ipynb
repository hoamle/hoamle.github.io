{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Nomenclature__\n",
    "* AE, DAE, VAE: (vanilla) auto-encoder, denoising auto-encoder, variational auto-encoder\n",
    "* RBM: Restricted Boltzmann Machine\n",
    "* DBN: Deep Belief Network i.e. \"hybrid\" stacked-RBMs\n",
    "* DBM: Deep Boltzmann Machine i.e. \"true\" stacked-RBMs\n",
    "* DBN-DNN: feed-forward DNN trained by back-prop, initialising with pre-trained equivalent DBN\n",
    "* $v$: observed data, or visible variable \n",
    "* $h$: latent/hidden variable "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think about the relevance of RBM/DBN in the current (\"current\" as in \"2014 onwards\") Machine Learning/Deep Learning research? Has the advent of VAE - variational auto-encoder [1], and effective regulariser such as Dropout made RBM/DBN ___obsolete___?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As opposed to vanilla AE and DAE, VAE is a fully _probabilistic_ generative model, which (also the most lucrative property) lets parameter $\\theta$ of each conditional distribution e.g. $\\text{Pr}\\left(h|v;\\theta\\right)=\\text{Pr}\\left(h|\\theta\\left(v\\right)\\right)$ be parameterised by a DNN. Nevertheless, the model still retains the merit of an AE against RBM: _not_ having to deal with partition function $Z$ in the objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Learning RBM and VAE both require maximising log-likelihood $L\\left(\\theta\\right)=\\log\\text{Pr}\\left(v;\\theta\\right)\n",
    " $, which boils down to estimating gradient $\\dfrac{\\partial L}{\\partial w}$ ($w$'s are elements of $\\theta$) by sampling. RBM's likelihood involves with computationally intractable partition function $Z$, thus  requires a form of Markov chain Monte Carlo (MCMC) estimation. On the other hand, VAE does not and requires simple Monte Carlo estimation.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mình có câu hỏi về vai trò của RBM/DBN/DBM cho nghiên cứu về representation learning hiện đại (từ 2014+), hi vọng được biết góc nhìn của mọi người về vấn đề này. Phải chăng RBM/DBN/DBM đã trở nên lỗi thời?\n",
    "(Chú thích: RBM - Restricted Boltzmann Machine, DBN - Deep Belief Network, DBM - Deep Boltzmann Machine)\n",
    "\n",
    "Xét với vai trò là mô hình sinh, VAE (Variational Auto-encoder) và stacked versions của loại mô hình này chỉ\n",
    "\n",
    "Xét về mặt mô hình và huấn luyện, VAE và RBM đều là generative and latent probabilistic models và tối ưu log-likelihood của dữ liệu. Tuy nhiên quá trình học tham số của VAE chỉ yêu cầu ước lượng bằng lấy mẫu Monte Carlo tiêu chuẩn, còn RBM cần ước lượng bằng MCMC do dính đến parition function Z\n",
    "\n",
    "Xét với vai trò là initialiser cho supervised DNN, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
