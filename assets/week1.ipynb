{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week, we will introduce **high-level overview of building a Machine Learning (ML) solution** for a problem, summarized by the following diagram and demo'ed with [`scikit-learn`](http://scikit-learn.org/stable/index.html).\n",
    "![](img/week1-2.png)\n",
    "\n",
    "\n",
    "***N.B.***: we will gradually *rephrase* every-day language with equivalent ML terminologies and their *math notations*. \n",
    "{% marginnote 'mn-id-glossary' \"Visit [MLglossary](https://hoamle.github.io/articles/17/machine-learning-appendix/#glossary) for a summary of the math notations. Because ML is an interdisplinary field, there are many ML terminologies that are equivalent. The MLglossary also lists common terms and their synonyms or strongly related concepts.\" %}\n",
    "\n",
    "Don't be afraid of Math, embrace it. Math is (1) an **efficient, universal and unanimously understood** language. Once we rephrase a problem in mathematical toungue, we may see hidden  connections between numerous of *supposedly* unrelated problems in different fields [[ref-DAslides-pp22-38](https://1drv.ms/b/s!ApOZHae4ogqZ3AJg76xtDPEzSlH-)]. Further more, Math is (2) **non-ambiguous** so that we can approach a solution of a problem with transparent and concrete reasoning. Nevertheless, always explain the math we use (or encounter) in our usual natural language if possible, so that we can imprint the **underlying intuition** and not getting lost in the technical details that follow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case-study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identify a flower type based on its attributes** which include petal and sepal size.*\n",
    "\n",
    "Ref: [http://scikit-learn.org/stable/tutorial/basic/tutorial.html](http://scikit-learn.org/stable/tutorial/basic/tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/Petal-sepal.jpg)\n",
    "(*\"which flower is this?\" - photo from [bing](https://www.bing.com/images/search?view=detailV2&ccid=E8dlW334&id=690CFCA2C5419961A77E0534DC3B6AD1B61FA711&q=sepal&simid=608048017631022285&selectedIndex=5&ajaxhist=0))*\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experience** i.e ***Training data***: `iris` dataset - a dataset that contain information about certain types of flower "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets \n",
    "iris_dataset = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, study our data, so that we can **pre-process** the data provided. In this step, we might need to manipulate the raw data to extract important information, or normalize following certain standard. The purpose is (but not limited to) to prepare the data in proper formats for later implementation.\n",
    "\n",
    "For demonstration purpose, we are using a \"*clean\"* dataset (already stored in structured format, no missing or suspicious values), so we only need to do minimal data pre-processing. Reminded that it is mostly *not* the case in practice [[ref-DAslides-p11](https://1drv.ms/b/s!ApOZHae4ogqZ3AJg76xtDPEzSlH-)]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the \"docs\" i.e. data description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris Plants Database\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Data Set Characteristics:\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# print(iris_dataset)\n",
    "print(iris_dataset['DESCR'][:1000])  # the \"doc\" is quite long, so I \n",
    "                                  # only extract the first 1000 \n",
    "                                  # characters for demonstration purpose    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are 150 examples i.e. *data points*\n",
    "* Each flower is described by $P=4$ attributes i.e. ***features*** or *predictors*: `sepal length` ($x_{1}$), `sepal width` ($x_{2}$), `petal length` ($x_{3}$), `petal width` ($x_{4}$). \n",
    "* All 4 features' measures are conceptually similar (numeric, real values that lie in the same domain), so no **data normalization** required.\n",
    "* There are $K=3$ types i.e. **classes** of flower, which are `setosa`, `versicolor`, and  `virginica` - represented by ID `0`, `1`, `2` respectively. Note: each flower is ***labelled*** by a *single* ID, indicating that a flower can only be member of 1 type.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150L, 4L)\n",
      "['setosa' 'versicolor' 'virginica']\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(iris_dataset['data'].shape)  \n",
    "# each row in `iris_dataset['data']` is \n",
    "# a feature vector of the respective flower\n",
    "\n",
    "print(iris_dataset['target_names'])\n",
    "print(iris_dataset['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write the features as a feature vector $x=\\left(x_{1},x_{2},x_{3},x_{4}\\right)^{T}$. Vector $x$ is a **representation** of a flower i.e. a summary of this flower attributes. We extract and store all feature vectors to a 2D-array $X$, where the $n$-th row corresponds to feature vector $x^{\\left(n\\right)}$. \n",
    "\n",
    "{% sidenote 'sn-id-2d' \"`2D-array` can be understood as a table-like data structure\" %}\n",
    "\n",
    "*<font color=\"red\">Note</font>*: In this course, we use capital letters e.g. $X$ to represent set of data points rather than a matrix as usual. If we imply the latter, it should be clear from the context.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150L, 4L)\n"
     ]
    }
   ],
   "source": [
    "D = {}\n",
    "D['X'] = iris_dataset['data']\n",
    "print(D['X'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract and store all the labels to an 1D-array $Y$, where the $n$-th element indicates flower type $y^{\\left(n\\right)}$. *<font color=\"red\">Note</font>*: the order of all $n$'s in $Y$ must match with the counterparts in $X$.\n",
    "\n",
    "{% sidenote 'sn-id-labels' \"In actual implementation (which we won't see in this week example), $Y$ is almost always converted to an equivalent 2D-array implicitly, where each column is a binary indicator (e.g. {1, 0}) of a respective class in all examples.\" %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150L,)\n"
     ]
    }
   ],
   "source": [
    "D['Y'] = iris_dataset['target']\n",
    "print(D['Y'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define our problem more precisely by the language of math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given any attribute $x$, we would like to **predict** a corresponding prediction $\\hat{y}\\in$  {`setosa`, `versicolor`, `virginica`} by a ML **Model**. In order to know if our model has good predictive performance, we need an **Assessment** to measure the performance.  In this problem, we can measure the performance by an ***evaluation metric*** called [*Accuracy*](http://scikit-learn.org/stable/modules/model_evaluation.html#accuracy-score), which\n",
    "\n",
    "$$\\text{Accuracy}=\\dfrac{1}{N}\\sum_{n}\\mathbb{1}\\left(\\hat{y}^{\\left(n\\right)}=y^{\\left(n\\right)}\\right)$$\n",
    "\n",
    "The value of this metric increases every time we correctly identify a flower type. A prediction is correct if predicted type $\\hat{y}$ of a flower is the same as its actual type $y$.\n",
    "\n",
    "![](img/week1-1.png)\n",
    "*<font color=\"red\">Note</font>*: while Accuracy is appropriate for this particular problem, other problems may require some other evaluation metric {% marginnote 'mn-id-metrics' \"Many problems require 2 or more metrics for an objective assessment.\" %} For example, in case wewant to identify whether a patient has cancer or not, we may want to minimize the risk of missing true cases (patients who actually have cancer - True Positive cases) as much as possible. In this case, [*Recall*](http://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-f-measure-metrics) is a better measure of performance than Accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In order to build a model that perform well, we need to **train** it by utilizing the *experience* that the model has. In other word, the model needs to improve its performance by *learning* from the *training data* $\\mathcal{D}_{\\text{train}}=\\left\\{ x^{\\left(n\\right)},y^{\\left(n\\right)}\\right\\}$ from an initial \"naive\" state. This task is called, unsurprisingly, ***learning***, and is also commonly called ***fitting***. \n",
    "![](img/week1-2.png)\n",
    "\n",
    "{% marginnote 'mn-id-knn' \"*k-nearest neighbours* - a common ML algorithm - is an exception such that the algorithm virtually does not perform any kind of learning. While the algorithm has its own mertis, it does not scale to address high-dimensional inputs or non-trivial semantic recognition problems. Read more: [[ref-cs231n](http://cs231n.github.io/classification/#nn)]\" %}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ML algorithm is not only required to perform well on the training data, but also on the *new, unseen data* that it has never encountered (i.e. not in the training set). Those unseen data that could be used to assess a model performance are called ***test data***. High performance on test data is what we aim for, because it is an indicator for the model's ability to  **generalize** to arbitrary data examples.\n",
    "\n",
    "For demonstration purpose, we will use a small subset of `iris` dataset as test data. The remaining will serve as training data.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set size: 30 examples\n"
     ]
    }
   ],
   "source": [
    "# Randomly pick 20% of the examples as test data, and train\n",
    "# on the remaining 80%.\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "# note: sklearn 0.18 moved `train_test_split` to \n",
    "# `sklearn.model_selection` module \n",
    "\n",
    "indicies = [i for i in xrange(len(iris_dataset['data']))]\n",
    "train_idx, test_idx = train_test_split(indicies, train_size=0.8,\n",
    "                                     random_state=1)  \n",
    "# Each time we run `train_test_split`, a new split is produced \n",
    "# randomly and is different from the previous run. Therefore, \n",
    "# it is essential to assign a `random_state` to make our work \n",
    "# reproducible on other machines or for later use. \n",
    "\n",
    "# Use capital letter e.g. `X` to indicate set of data points.\n",
    "D_train = {'X': D['X'][train_idx],\n",
    "           'Y': D['Y'][train_idx]\n",
    "           }\n",
    "D_test = {'X': D['X'][test_idx],\n",
    "         'Y': D['Y'][test_idx]\n",
    "        }\n",
    "print(\"Test set size: {} examples\".format(len(test_idx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this week example demo, we use and train a model called ***Softmax Regression***. For now, we can simply understand Softmax Regression as a \"black-box\" ML algorithm that can do our prediction problem. *\"What are assumptions on model structure? How was learning task done? Can we do better than standard Softmax Regression?, etc\"* are the topics for next weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# Softmax regression is a multi-class generalization of \n",
    "# Logistic Regression model\n",
    "\n",
    "# initialize a \"naive\" model\n",
    "model = LogisticRegression(multi_class=\"multinomial\",\n",
    "                          solver=\"lbfgs\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(D_train['X'], D_train['Y']);  # after `.fit`, the model has finished its learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning finished! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "Now try indentifying a new, unseen flower with feature $x^{\\left(\\text{test}\\right)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.8  4.   1.2  0.2]\n"
     ]
    }
   ],
   "source": [
    "x_test = D_test['X'][0]\n",
    "print(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hoa\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)  \n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model `predict`s that the flower whose petal and sepal size are as described by $x^{\\left(\\text{test}\\right)}$ is a \"setosa\" (`id: 0`). ***Is this true?***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred == D_test['Y'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good news! But what about prediction for other flowers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flower\tPredict\tActual\tCorrect prediction?\n",
      "14\t0\t0\tTrue\t\n",
      "98\t1\t1\tTrue\t\n",
      "75\t1\t1\tTrue\t\n",
      "16\t0\t0\tTrue\t\n",
      "131\t2\t2\tTrue\t\n",
      "56\t1\t1\tTrue\t\n",
      "141\t2\t2\tTrue\t\n",
      "44\t0\t0\tTrue\t\n",
      "29\t0\t0\tTrue\t\n",
      "120\t2\t2\tTrue\t\n",
      "94\t1\t1\tTrue\t\n",
      "5\t0\t0\tTrue\t\n",
      "102\t2\t2\tTrue\t\n",
      "51\t1\t1\tTrue\t\n",
      "78\t1\t1\tTrue\t\n",
      "42\t0\t0\tTrue\t\n",
      "92\t1\t1\tTrue\t\n",
      "66\t1\t1\tTrue\t\n",
      "31\t0\t0\tTrue\t\n",
      "35\t0\t0\tTrue\t\n",
      "90\t1\t1\tTrue\t\n",
      "84\t1\t1\tTrue\t\n",
      "77\t2\t1\tFalse\t\n",
      "40\t0\t0\tTrue\t\n",
      "125\t2\t2\tTrue\t\n",
      "99\t1\t1\tTrue\t\n",
      "33\t0\t0\tTrue\t\n",
      "19\t0\t0\tTrue\t\n",
      "73\t1\t1\tTrue\t\n",
      "146\t2\t2\tTrue\t\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(D_test['X'])\n",
    "\n",
    "print(\"Flower\\tPredict\\tActual\\tCorrect prediction?\")\n",
    "for i in xrange(len(test_idx)):\n",
    "    print(\"{}\\t{}\\t{}\\t{}\\t\".format(\n",
    "            test_idx[i], \n",
    "            Y_pred[i],\n",
    "            D_test['Y'][i],\n",
    "            Y_pred[i] == D_test['Y'][i]\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model correctly indentifies 29 examples and makes 1 errors out of 30 testing examples, achieving an Accuracy of $\\dfrac{1}{N}_{\\text{test}}\\sum_{n}\\mathbb{1}\\left(\\hat{y}^{\\left(n\\right)}=y^{\\left(n\\right)}\\right)=29/30=96.7\\%$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96666666666666667"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(Y_pred == D_test['Y']) / float(len(test_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(comment: The performance is pretty impressive, but let hold off from a pre-matured assessment! See [Conclusion](#conclusion))*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a href=\"#conclusion\">Conclusion</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, we have completed the most basic steps to build a ML solution for our problem as specified by this diagram\n",
    "![](img/week1-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, we studied a (single-label) `multi-class classification` problem which require us to build a ML model that can predict a flower type based on its sepal and petal measurements. As a solution for this problem, we used and trained a Softmax Regression `Model`, did `Assessment` on a *single* split of test set, and achieved impressive Accuracy of $96.7\\%$. However, there still exist *fundamental* questions that we need to address:\n",
    "\n",
    "1. Is $96.7\\%$ a *reliable* estimate for our model accuracy on **other test sets**?{% sidenote 'sn-id-assess' \"TODO link to Lecture 4\" %}\n",
    "2. How is Softmax Regression model **constructed**?{% sidenote 'sn-id-buildmodel' \"TODO link to Lecture 2\" %}\n",
    "3. In a problem for which **assumptions** imposed by Softmax Regression model do **not suffice**, how can we do better?{% sidenote 'sn-id-nonlinear' \"TODO link to Lecture 3\" %}\n",
    "4. *(other issues that would arise when approaching above questions)*\n",
    "\n",
    "\n",
    "These questions will define the topics for next parts of this series."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
